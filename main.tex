\documentclass[11pt,letterpaper]{article}
\usepackage[left=3.5cm,right=3.5cm,top=2.5cm,bottom=2.5cm]{geometry}
%\usepackage[spanish]{babel}

\usepackage{amsmath,amsfonts,amsthm}
\usepackage{enumitem,mathtools}
\usepackage[english]{isodate}
\setenumerate[0]{label=(\alph*)}

\newtheorem{definition}{Definición}
\newtheorem{exercise}{Ejercicio}
\newtheorem{example}{Ejemplo}
\newtheorem{theorem}{Teorema}
\newtheorem{remark}{Observación}
\newtheorem*{sol}{Solución}

\newcommand\N{\mathbb N}

\title{Notas de análisis numérico para ecuaciones diferenciales}
\author{Jorge Alfredo Álvarez Contreras}

\begin{document}
\maketitle

\section{Métodos de un paso}

\subsection{Estabilidad absoluta}

\begin{example}
  Consideremos el PVI
  \begin{equation}
    \left\{
    \begin{aligned}
      y' &= -2y \\
      y(0) &= 1
    \end{aligned}
    \right.
  \end{equation}
  La solución es, claramente, $y(t)=e^{-2t}$.

  El método de Euler es
  \begin{equation}
    \left\{
    \begin{aligned}
      u_{n+1} &= u_n + hf(t_n,u_n) \\
      u_0 &= 1
    \end{aligned}
    \right.
  .\end{equation}
  Es decir, la función de incremento es simplemente $\Phi=f$.
  Como $\Phi(t,y)=f(t,y)=-2y$ es Lipschitz en su segundo argumento
  podemos concluir que el método es consistente (de orden $1$).
  También es convergente.
  \begin{equation}
    \left\{
    \begin{aligned}
      u_{n+1} &= u_n + -2u_nh = (1-2h)u_n \\
      u_0 &= 1
    \end{aligned}
    \right.
  .\end{equation}
  

\end{example}

\begin{remark}
  No todos los métodos implicitos son inconcidionalmente estables.
\end{remark}

\subsection{Ecuaciones en diferencias }

Consideremos ecuaciones en diferencias de la forma
\begin{equation}\label{eq:ec_diferencias}
  u_{n+k} + \alpha_{k-1}u_{n+k-1}
  + \dots +
  \alpha_1 u_{n+1}+\alpha_0u_n
  =
  \phi_{n+k},
  \quad \alpha_0 \neq 0; n=0,1,\dots
\end{equation}
(dados $u_0,u_1,\dots,u_{k-1}$ ).
Si $\phi_{n+k}=0,n=0,1,\dots$, decimos que la ecuación en diferencias
es homogénea. Si $\alpha_{k-1},\alpha_{k-2},\dots,\alpha_0$ son
constantes, decimos que es una ecuación en diferencias con
coeficientes constantes.

Dados los valores iniciales $u_0,u_1,\dots,u_{k-1}$, se puede
determinar $u_n$ directamente a partir de la ecuación en diferencias,
sustituyendo sucesivamente.

Consideremos el problema homogéneo y busquemos soluciones de la forma
$u_n=r^{n}$. Entonces
\begin{equation}
  r^{n+k} + \alpha_{k-1}r^{n+k-1}
  + \dots +
  \alpha_1 r^{n+1}+\alpha_0r^n
  =
  0.
\end{equation}
Para $r=0$, obtenemos la solución trivial, así que supondremos que
$r\neq 0$, de modo que al cancelar $r^{n}$ obtenemos
\begin{equation}
  r^{k} + \alpha_{k-1}r^{k-1}
  + \dots +
  \alpha_1 r^{1}+\alpha_0
  =
  0.
\end{equation}
El lado izquierdo de esta ecuación se es un polinomio de grado $k$ y
se llama el polinomio característico de la ecuación en diferencias
\eqref{eq:ec_diferencias} 
Se denota con $\pi(r)$.
Si $\pi(r)$ tiene $k$ raíces distintas, digamos
$r_0,r_1,\dots,r_{k-1}$, entonces la solución general de la ecuación
en diferencias es
\begin{equation}\label{eq:sol_general}
  u_n = \gamma_0r_0^{n} + \dots + \gamma_{k-1}r_{k-1}^{n}
\end{equation}
y, dados $u_0,\dots,u_{k-1}$, existe un conjunto único de constantes
$\gamma_0,\dots,\gamma_{k_1}$ tal que \eqref{eq:sol_general}.

\begin{example}
  La sucesión de Fibonacci es la solución a la ecuación
  \begin{equation}
    u_{n+2} = u_{n+1} + u_n
  \end{equation}
  con condiciones iniciales $u_0=u_1=1$.
  Las soluciones de la forma $u_n=r^n$ satisfacen la ecuación
  característica
  \begin{equation}
    r^2 = r + 1
  .\end{equation}
  Es decir,
  \begin{equation}
    r = \frac{1\pm\sqrt 5}{2}
  .\end{equation}
  Por lo tanto,
  \begin{equation}
    u_n = \gamma_0
    \left( \frac{1+\sqrt 5}{2} \right)^n
    +
    \gamma_1
    \left( \frac{1-\sqrt 5}{2} \right)^n
  ,\end{equation}
  donde las constantes $\gamma_0$ y $\gamma_1$ se obtienen aplicando
  las condiciones iniciales
  \begin{align}
    1 = u_0 &= \gamma_0
    +
    \gamma_1
    \\
    1 = u_1 &= \gamma_0
    \left( \frac{1+\sqrt 5}{2} \right)
    +
    \gamma_1
    \left( \frac{1-\sqrt 5}{2} \right)
  ,\end{align}
  Resolviendo este sistema, obtenemos
  \begin{equation}
    \gamma_{0,1} = \frac{1}{2} \pm \frac{\sqrt 5}{10}
  .\end{equation}
\end{example}

Si la raíz $r_j$ tiene multiplicidad $m\geq 2$, debemos buscar
soluciones de la forma $p(n)r^n$, donde $p$ es un polinomio en $n$ de
grado $m-1$.

\begin{example}
  Consideremos la ecuación
  \begin{equation}
    u_{n+3} -2u_{n+2} - 7 u_{n+1} - 4u_n = 0
  \end{equation}
  con condiciones iniciales $u_0=1$, $u_1=0$, $u_{2}=-1$.
  La ecuación característica
  \begin{equation}
    r^{3}-2r^{2}-7r-4 = 0
  \end{equation}
  tiene una raíz simple $r_0=4$ y una raíz doble $r_{1,2}=-1$.
  Por lo tanto, la solución general es
  \begin{equation}
    u_n = \gamma_{0}4^n + (\gamma_1+\gamma_2n)(-1)^n
  .\end{equation}
  Aplicando las condiciones iniciales, obtenemos ecuaciones
  \begin{align}
    \gamma_0+\gamma_1 &= 1 \\
    4\gamma_0-\gamma_1-\gamma_2 &= 0 \\
    16\gamma_0+\gamma_1+2\gamma_2&= -1
  \end{align}
  con solución $\gamma_0=0$, $\gamma_1=1$, $\gamma_2=-1$.
  Por lo tanto, la solución es
  \begin{equation}
    u_n = (1-n)(-1)^{n}
  .\end{equation}
\end{example}

\section{Métodos multipaso}

\section{Métodos Adams}

El problema de valor inicial
\begin{equation}
  \left\{
    \begin{aligned}
      y'(t) &= f(t,y(t)) \\
      y(t_0) &= y_0
    \end{aligned}
  \right.
\end{equation}
en $I=[t_0,t_N]$, es equivalente a las ecuaciones integrales
\begin{equation}
  y(t_{n+1}) = y(t_n) + \int_{t_n}^{t_{n+1}}\! f(t,y(t))\,dt
\end{equation}
para $n=0,\dots,N-1$.

La aproximación se construye como
\begin{equation}
  u_{n+1} = u_n + \int_{t_n}^{t_{n+1}}\Pi(t)\,dt,
\end{equation}
donde $\Pi(t)$ es el polinomio interpolador de $f(t,y(t))$.


\subsection{Metodos Adams-Bashforth (explícitos)}
Interpolar en $t_{n},t_{n-1},\dots,t_{n-p}$.

\begin{example}
  Recordemos que el polinomio interpolador de Lagrange de grado $n$ 
  con datos $(x_0,y_0),(x_1,y_1),\dots,(x_n,y_n)$ se construye como
  \begin{equation}
    \Pi(x) =  L_0(x)y_0 + L_1(x)y_1 + \dots + L_n(x)y_n
  \end{equation}
  donde cada $L_i(x)$ es un polinomio de grado $n$ que satisface
  $L_i(x_j)=\delta_{ij}$. Explícitamente,
  \begin{equation}
    L_i(x) = \frac{\prod_{j\neq i}(x-x_j)}{\prod_{j\neq i}(x_i-x_j)}
  .\end{equation}
\end{example}

\begin{example}
  Interpolaremos $f(t,y(t))$ en $f_n,f_{n-1},f_{n-2}$. Entonces
  \begin{align}
    \Pi(t)
    &= \frac{(t-t_{n-1})(t-t_n)}{(-h)(-2h)}f_{n-2}
    +  \frac{(t-t_{n-2})(t-t_n)}{(h)(-h)}f_{n-1}
    +  \frac{(t-t_{n-2})(t-t_{n-1})}{(2h)(h)}f_{n}
    \\
    &=
    \frac{1}{2h^{2}}
    \left[
      (t-t_{n-1})(t-t_n)f_{n-2}
    - 2(t-t_{n-2})(t-t_n)f_{n-1}
    + (t-t_{n-2})(t-t_{n-1})f_{n}
    \right]
  .\end{align}
  Cualquier polinomio $f(t)$ de grado $2$ se puede integrar como
  \begin{equation}
    \int_{a}^{b}f(t)\,dt = \frac{b-a}{3}[f(a)+4f((a+b)/2)+f(b)]
  .\end{equation}
  Así,
  \begin{align}
     \int_{t_n}^{t_{n+1}}\Pi(t)\,dt
     =
     \frac{h}{6}[\Pi(t_{n})+4\Pi(t_{n}+h / 2)+\Pi(t_{n+1})]
  .\end{align}
\end{example}

\subsection{Métodos Adams-Moulton (implícitos)}
Interpolar con $t_{n+1},t_n,\dots,t_{n-p}$.

\begin{example}
  Supongamos que $\Pi(t)$ es el polinomio interpolador únicamente en
  un punto: $t_{n+1}$, es decir, $\Pi(t)=f_{n+1}$.
  Por lo tanto, el método
  \begin{equation}
    u_{n+1} = u_n + \int_{t_n}^{t_{n+1}} \Pi(t) \,dt
  \end{equation}
  se reduce a
  \begin{equation}
    u_{n+1} = u_n + hf_{n+1}.
  \end{equation}

  Para el caso de dos puntos, interpolaremos en $t_{n+1},t_n$:
   \begin{equation}
    \Pi(t)
    = \frac{t-t_{n+1}}{t_{n}-t_{n+1}}f_n
    + \frac{t-t_n}{t_{n+1}-t_n}f_{n+1}
  .\end{equation}
  Como $\Pi(t)$ es de grado $1$, podemos evaluar la integral con la
  regla trapezoidal (Simpson $\frac{1}{2}$) como sigue:
  \begin{align}
    u_{n+1}
    &= u_n + \int_{t_n}^{t_{n+1}} \Pi(t) \,dt \\
    &= u_{n} + \frac{h}{2}(\Pi(t_{n})+\Pi(t_{n+1})) \\
    &= u_{n} + \frac{h}{2}(f_{n}+f_{n+1}).
  \end{align}

  Para el caso de tres puntos, interpolaremos en $t_{n+1},t_n,t_{n-1}$ 
  como sigue:
  \begin{equation}
    \begin{split}
      \Pi(t)
      &=
      \frac{(t-t_{n})(t-t_{n+1})}{(t_{n-1}-t_n)(t_{n-1}-t_{n+1})}
      f_{n-1}
      +
      \frac{(t-t_{n-1})(t-t_{n+1})}{(t_{n}-t_{n-1})(t_{n}-t_{n+1})}
      f_{n}
      \\
      &\quad +
      \frac{(t-t_{n-1})(t-t_{n})}{(t_{n+1}-t_{n-1})(t_{n+1}-t_{n})}
      f_{n+1}
      \\
      &=
      \frac{(t-t_{n})(t-t_{n+1})}{(-h)(-2h)}
      f_{n-1}
      +
      \frac{(t-t_{n-1})(t-t_{n+1})}{(h)(-h)}
      f_{n}
      +
      \frac{(t-t_{n-1})(t-t_{n})}{(2h)(h)}
      f_{n+1}
      \\
      &=
      \frac{1}{2h^{2}}
      \left[
        (t-t_{n})(t-t_{n+1}) f_{n-1}
        - (t-t_{n-1})(t-t_{n+1}) f_{n}
        + (t-t_{n-1})(t-t_{n}) f_{n+1}
      \right]
    \end{split}
  \end{equation}
  Ahora $\Pi(t)$ es un polinomio de grado $2$, así que podemos
  evaluarlo con la regla de Simpson $\frac{1}{3}$ como sigue:
  \begin{align}
    u_{n+1}
    &= u_n + \int_{t_n}^{t_{n+1}} \Pi(t) \,dt \\
    &= u_{n}
    + \frac{h}{6}(\Pi(t_{n})+4\Pi(t_n + h / 2)+\Pi(t_{n+1})) \\
    &= u_{n}
    + \frac{h}{6}(f_{n}+4\Pi(t_n + h / 2)+f_{n+1}) \\
    &= 
  \end{align}
\end{example}

\subsection{Métodos BDF (backwards difference formulae)}

Dado el problema
\begin{equation}
  y'(t)=f(t,(t))
,\end{equation}
la idea es aproximar $y'(t_{n+1})$ con diferencias finitas atrasadas
de distintos órdenes (usando $t_{n+1},t_{n},t_{n-1},\dots,t_{n-p}$).

\begin{example}
  \begin{enumerate}
    \item
      Usando $t_{n+1},t_{n},t_{n-1}$, aproximar $y'(t_{n+1})$:
      \begin{equation}
        y'_{n+1} = ay_{n-1} + by_{n} + cy_{n+1}
      .\end{equation}
      Esto es
      \begin{equation}
        y'(t_{n}+h)
        = ay(t_{n}-h) + by(t_{n}) + cy(t_n+h)
      .\end{equation}
      Expandiendo en serie de Taylor hasta orden $2$, esto es
      \begin{align}
        &\hspace{-10mm}
        y'_n + hy''_n + \frac{h^{2}}{2}y'''_n + O(h^{3}) \\
        &= a \left[y_n - h y'_n + \frac{h^{2}}{2}y''_n\right]
        + b y_n
        + c \left[y_n + hy'_n + \frac{h^{2}}{2}y''_n\right]
        + O(h^{3})
      .\end{align}
      Luego, obtenemos el sistema de ecuaciones
      \begin{align}
        a+b+c &= 0, \\
        -ah+ch &= 1, \\
        \frac{ah^{2}}{2} + \frac{ch^{2}}{2} &= h,
      \end{align}
      que tiene solución
      \begin{align}
        a &= \frac{1}{2h}, &
        b &= -\frac{2}{h}, &
        c &= \frac{3}{2h}.
      \end{align}
      Aplicando la aproximación, esto es
      \begin{align}
        y'_{n+1}
        &= \frac{1}{2h}y_{n-1} - \frac{2}{h}y_n + \frac{3}{2h}y_{n+1}
        + O(h^{3})
        \\
        &= \frac{1}{2h}[y_{n-1} - 4y_n + 3y_{n+1}] + O(h^{3})
      .\end{align}
      Luego, obtenemos el método
      \begin{equation}
        f_{n+1} = \frac{1}{2h}[u_{n-1} - 4u_n + 3u_{n+1}]
      .\end{equation}
  \end{enumerate}
\end{example}o

\subsection{Análisis de los métodos multipaso}
El método multipaso
\begin{equation}
  u_{n+1}
  =
  \sum_{j=0}^{p}a_ju_{n-j}
  +
  h
  \sum_{j=0}^{p}b_jf_{n-j} + b_{-1}f_{n+1},
  \quad
  n=p, p+1,\dots
\end{equation}
tiene error de truncamiento local
\begin{equation}
  \tau_{n+1}
  =
  \frac{1}{h}
  \left[
    y_{n+1}
    -
    \sum_{j=0}^{p} a_jy_{n-j}
    +
    h
    \sum_{j=1}^{p}b_{j}y'_{n-j}
    +
    b_{-1}f_{n+1}
  \right]
,\end{equation}
donde $y_{n-j}=y(t_{n-j})$,
$y'_{n-j}=y'(t_{n-j})=f(t_{n-j},y(t_{n-j}))$.
El error de truncamiento global es
\begin{equation}
  \tau(h) = \max_n |\tau_{n}(h)|
.\end{equation}

El método es consistente si $\lim_{h\to 0}\tau(h)=0$.
El método tiene orden $q$ si $\tau(h)=O(h^{q})$ para algún $q\geq 1$.
Si el método tiene orden $q$, entonces
\begin{equation}
  \tau_{n+1}(h) = C_qh^qy^{(q+1)}(t_n) + O(h^{q+1})
.\end{equation}
La constante $C_q$ se llama constante de error.

\begin{example}
  Método de Euler:
  \begin{equation}
    u_{n+1} = u_{n} + hf_n
  .\end{equation}
  Entonces
  \begin{align}
    \tau_{n+1}
    &= \frac{1}{h}(y_{n+1} - (y_n+hy'_n)) \\
    &= \frac{1}{h}
    \left(
    y_n + hy'_n + \frac{h^{2}}{2} y''_n + O(h^{3})
    - (y_n + hy'_n))
    \right)
    \\
    &= \frac{h}{2}y''_n + O(h^{3})
  .\end{align}
  Así, el método de Euler es de orden $1$ y la constante de error es
  $C_1=\frac{1}{2}$.
\end{example}

\begin{theorem}
  El método multipaso es consistente si se cumplen las condiciones
  \begin{equation}\label{eq:consistencia_multi}
    \sum_{j=0}^{p}a_j = 1,
    \qquad
    -\sum_{j=0}^{p}ja_j + \sum_{j=-1}^{p}b_j = 1.
  \end{equation}
  Más aún, si $y\in C^{q+1}(I)$ para algún $q\geq 1$, entonces el
  método es de orden $q$ si, y solo si, se satisfacen
  \eqref{eq:consistencia_multi} y
  \begin{equation}
    \sum_{j=0}^{p}(-j)^{i}a_j + i \sum_{j=-1}^{p}(-j)^{i-1}b_j = 1,
    \quad i=2,3,\dots,q
  .\end{equation}
\end{theorem}
\begin{proof}
  (Solo para la parte de consistencia).

  Primero, observemos que
  \begin{equation}
    y_{n-j} = y_{n} - jhy'_n + O(h^{2}),
    \qquad 
    y'_{n-j} = y'_n + O(h)
  .\end{equation}
  Así,
  \begin{align}
    h\tau_{n+1}
    &= y_{n+1}
    - \sum_{j=0}^{p}a_jy_{n-j}
    - h \sum_{j=-1}^{p}b_{j}y'_{n-j} \\
    &= y_{n+1}
    - \sum_{j=0}^{p}a_j(y_n - jhy'_n + O(h^{2}))
    - h \sum_{j=-1}^{p}b_{j}(y'_{n}+O(h)) \\
    &= y_{n+1}
    - y_n \sum_{j=0}^{p}a_j
    + hy'_n \sum_{j=0}^{p}ja_j
    - hy'_n \sum_{j=-1}^{p}b_{j} + O(h^{2})
    \\
    &= y_{n+1} - y_n
    + y_n\left(1-\sum_{j=0}^{p}a_j\right)
    - hy'_n
    \left( - \sum_{j=0}^{p}ja_j + \sum_{j=-1}^{p}b_{j} \right)
    + O(h^{2})
  .\end{align}
  
  Si se cumplen las hipotesis del teorema, el primer
  paréntesis es $0$ y el segundo es $1$, de modo que
  \begin{equation}
    \tau_{n+1}
    = \frac{y_{n+1} - y_n}{h} - y'_n + O(h)
    = \frac{y_{n+1} - y_n}{h} - y'_n + O(h)
    \to 0
  \end{equation}
  cuando $h\to 0$.
\end{proof}

\subsection{Condiciones de la raíz}

El problema de prueba
\begin{equation}
  \left\{
    \begin{aligned}
      y' &= \lambda y, \\
      y(0) &= 1.
    \end{aligned}
  \right.
\end{equation}
tiene $f(t,y)=\lambda y$. Aplicando el método multipaso, obtenemos
\begin{equation}
  u_{n+1}
  =
  \sum_{j=0}^{p}a_ju_{n-j}
  +
  h\lambda
  \sum_{j=-1}^{p}b_ju_{n-j}
  \quad
  n=p, p+1,\dots
\end{equation}
la cual es una ecuación en diferencias con coeficientes constantes.
Su polinomio característico
\begin{equation}
  \pi(r)
  = 
  r^{n+1}
  -
  \sum_{j=0}^{p}a_jr^{n-j}
  -
  h\lambda
  \sum_{j=-1}^{p}b_jr^{n-j}
\end{equation}
se puede expresar en la forma
\begin{equation}
  \pi(r) = p(r) - h\lambda\sigma(r)
\end{equation}
donde
\begin{equation}
  \rho(r)
  =
  r^{p+1}
  -
  \sum_{j=0}^{p}a_jr^{p-j},
  \qquad
  \sigma(r)
  =
  \sum_{j=-1}^{p}b_jr^{p-j}
.\end{equation}

Si $r_i(h\lambda)$ son las raíces de $\pi(r;h\lambda)$, entonces las
soluciones fundamentales son de la forma $u_n = [r_i(h\lambda)]^n$.

\begin{definition}
  El método satisface la \emph{condición absoluta de la raíz} si
  existe $h_0>0$ tal que, para $h\in (0,h_0]$, entonces
  \begin{equation}
    |r_j(h\lambda)|<1, \quad j=0,1,\dots,p
  .\end{equation}
\end{definition}

\begin{definition}
  Sean $r_j=r_j(0)$ las raíces de $\rho(r)$.
  El método multipaso satisface la \emph{condición de la raíz} si
  todas las
  raíces $r_i$ están contenidas en el disco unitario (cerrado) con
  centro en el origen y, si estas caen en la frontera, entonces
  deberán ser simples. I.e.
  \begin{equation}
    |r_j|\leq 1, \quad j=0,\dots,p
    \quad \text{y} \quad
    \text{si $|r_j|=1$, entonces } \rho'(r_j)\neq 0
  .\end{equation}
\end{definition}
\begin{definition}
  El método multipaso satisface la \emph{condición fuerte de la raíz}
  si satisface la condición de la raíz y $r_0=1$ es la única raíz que
  está en la frontera del disco unitario.
\end{definition}

\begin{definition}[Cero-estabilidad para métodos multipaso]
  Consideremos un método multipaso y una perturbación por
  $\delta_{n}$
  \begin{align}
    &\left\{
      \begin{aligned}
        u_{n+1}
        &= \sum_{j=0}^{p}a_ju_{n-j}
        + h \sum_{j=-1}^{p} b_jf_{n-j} \\
        u_k &= w_k, \quad k=0,1,\dots,p
      \end{aligned}
    \right.
    \label{eq:metodosinperturbar}
    \\
    &\left\{
      \begin{aligned}
        z_{n+1}
        &= \sum_{j=0}^{p}a_jz_{n-j}
        + h \sum_{j=-1}^{p} b_jf(t_{n-j},z_{n-j}) + h\delta_{n+1} \\
        z_k &= w_k + \delta_k, \quad k=0,1,\dots,p
      \end{aligned}
    \right.
  .\end{align}
  Decimos que el método \eqref{eq:metodosinperturbar}  
  es cero-estable si existen $h_0,C>0$ tales
  que, para $h\in(0,h_0]$
  \begin{equation}
    |z_n-u_n|\leq C\epsilon, \quad 0\leq n\leq N_h
  \end{equation}
  siempre que $|\delta_n|<\epsilon, 0\leq n\leq N_h$.
\end{definition}

\begin{theorem}[Equivalencia de estabilidad cero y la condición de la
  raíz]
  Para un método multipaso consistente, la condición de la raíz es
  equivalente a la estabilidad cero.
\end{theorem}

\begin{theorem}
  Un método multipaso consistente es convergente si, y solo si,
  satisface la condición de la raíz y el error en los valores
  iniciales tiende a cero conforme $h$ tiende a cero. Más aún, el
  método converge con orden $q$ si $\tau(h)=O(h^q)$ y el error en los
  datos iniciales tiende a cero como $O(h^q)$.
\end{theorem}

\end{document}
