\documentclass[11pt,letterpaper]{report}
\usepackage[left=3.5cm,right=3.5cm,top=2.5cm,bottom=2.5cm]{geometry}
%\usepackage[spanish]{babel}

\usepackage{amsmath,amsfonts,amsthm}
\usepackage{enumitem,mathtools,hyperref}
\usepackage[english]{isodate}
\setenumerate[0]{label=(\alph*)}

\newtheorem{definition}{Definición}
\newtheorem{exercise}{Ejercicio}
\newtheorem{example}{Ejemplo}
\newtheorem{theorem}{Teorema}
\newtheorem{remark}{Observación}
\newtheorem*{sol}{Solución}

\newcommand\N{\mathbb N}
\newcommand\R{\mathbb R}
\newcommand\ol\overline
\renewcommand\phi\varphi
\renewcommand\contentsname{Contenido}
\renewcommand\chaptername{Capítulo}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=cyan,
    citecolor=blue,     
    urlcolor=cyan,
    pdftitle={tesis-jorge-alfredo-conjuntos-sobre-marcos},
    }

\title{Análisis numérico para ecuaciones diferenciales
  \\ \vspace{5mm}
\large Notas de clase}
\author{Jorge Alfredo Álvarez Contreras}

\begin{document}
\maketitle
\tableofcontents

\section*{Referencias}

\begin{itemize}
  \item
    Numerical mathematics - A. Quarteroni
  \item
    Numerical analysis - Burden
  \item
    Finite differences for differential equations - LeVeque
  \item
    Theory of ordinary differential equations - Coddington
  \item
    P.D.E in mathematical physics - Evans
  \item
    P.D.E in action - Salsa
\end{itemize}

\chapter{Problemas de valores iniciales de E.D.O.}

El problema a tratar es
\begin{equation}
  \left\{
    \begin{aligned}
      y'(t) &= f(t,y(t)), \quad t \in [t_0,t_0+T]
      \\
      y(t_0) &= y_0
    \end{aligned}
  \right.
\end{equation}

\section{Métodos de un paso}

Un método es explícito si $u_{n+1}$ solo depende de $u_n$.

Cualquier método explícito de un paso se puede expresar como
\begin{equation}
  u_{n+1} = u_n + h \Phi(t_n,u_n,f_n;h)
,\end{equation}
donde $\Phi$ se conoce como la función de incremento.
El \emph{error residual} $\epsilon_{n+1}$ se define por la ecuación
\begin{equation}
  y_{n+1}
  =
  y_n + h\Phi(t_n,y_n,f(t_n,y_n);h) + \epsilon_{n+1}
,\end{equation}
donde $y_n = y(t_n)$ son los valores en $t_n$ de la solución exacta
$y(t)$.
Reacomodando esta ecuación, tenemos
\begin{equation}
  \frac{y_{n+1} - y_n}{h}
  =
  \Phi(t_n,y_n,f(t_n,y_n);h) + \frac{\epsilon_{n+1}}{h}
.\end{equation}
Al término $\tau_{n+1}(h)=\epsilon_{n+1}/h$ se le conoce como
\emph{error de truncamiento local}. Es decir,
\begin{equation}
  \tau_{n+1}(h)
  =
  \frac{y_{n+1} - y_n}{h} -\Phi(t_n,y_n,f(t_n,y_n);h)
.\end{equation}
El error de truncamiento es
\begin{equation}
  \tau(h) = \sup_{0\leq n\leq N_h} |\tau_{n}(h)|
,\end{equation}

\subsection{Consistencia}

Decimos que un método es consistente si
\begin{equation}
  \lim_{h\to 0}\tau(h) = 0
.\end{equation}
Más específicamente, decimos que un método consistente tiene orden
$p$ si
\begin{equation}
  \tau(h) = O(h^p) \quad \text{cuando} \quad h\to 0
.\end{equation}

\begin{example}
  Para el método de Euler
  \begin{equation}
    u_{n+1} = u_n + hf_n
  ,\end{equation}
  tenemos $\Phi = f$, así que
  \begin{align}
    \tau_{n+1}(h)
    &= \frac{y_{n+1}-y_n}{h}-\Phi(t_n,y_n,f(t_n,y_n);h) \\
    &= \frac{y_{n+1}-y_n}{h}-f(t_n,y_n) \\
    &= \frac{y_{n+1}-y_n}{h}-y'_n \\
    &= \frac{hy'_n + O(h^2)}{h}-y'_n \\
    &= y'_n + O(h)-y'_n \\
    &= O(h)
  ,\end{align}
  de modo que el método tiene orden $1$.
\end{example}

\begin{example}
  El método
  \begin{equation}
    u_{n+1} = u_n + \frac{h}{2}
    \left[ f_n + f(t_{n+1},u_n + hf_n) \right]
  \end{equation}
  tiene orden $2$.
\end{example}

\subsection{Estabilidad cero}

Consideremos el método de un paso
\begin{equation}
  \left\{
    \begin{aligned}
      u_{n+1} &= u_n + h\Phi(t_n,u_n,f_n;h),
      \\
      u_0 &= y_0.
    \end{aligned}
  \right.
\end{equation}
Si introducmos perturbaciones $\delta_0,\dots,\delta_{N_h}$,
obtendremos un método perturbado
\begin{equation}
  \left\{
    \begin{aligned}
      z_{n+1} &= z_n + h[\Phi(t_n,z_n,f(t_n,z_n);h) + \delta_{n+1}],
      \\
      z_0 &= y_0 + \delta_0.
    \end{aligned}
  \right.
\end{equation}
Decimos que el método es \emph{cero-estable} si, para todo
$\epsilon>0$ existen $h_0,C>0$ tales que, si las perturbaciones
cumplen $|\delta_n|\leq\epsilon$ y $0<h\leq h_0$, entonces
\begin{equation}
  |z_n - u_n| \leq C\epsilon, \quad n=0,1,\dots,N_h
.\end{equation}


\subsection{Estabilidad absoluta}

\begin{example}
  Consideremos el PVI
  \begin{equation}
    \left\{
    \begin{aligned}
      y' &= -2y \\
      y(0) &= 1
    \end{aligned}
    \right.
  \end{equation}
  La solución es, claramente, $y(t)=e^{-2t}$.

  El método de Euler es
  \begin{equation}
    \left\{
    \begin{aligned}
      u_{n+1} &= u_n + hf(t_n,u_n) \\
      u_0 &= 1
    \end{aligned}
    \right.
  .\end{equation}
  Es decir, la función de incremento es simplemente $\Phi=f$.
  Como $\Phi(t,y)=f(t,y)=-2y$ es Lipschitz en su segundo argumento
  podemos concluir que el método es consistente (de orden $1$).
  También es convergente.
  \begin{equation}
    \left\{
    \begin{aligned}
      u_{n+1} &= u_n + -2u_nh = (1-2h)u_n \\
      u_0 &= 1
    \end{aligned}
    \right.
  .\end{equation}
  

\end{example}

\begin{remark}
  No todos los métodos implicitos son inconcidionalmente estables.
\end{remark}

\subsection{Ecuaciones en diferencias }

Consideremos ecuaciones en diferencias de la forma
\begin{equation}\label{eq:ec_diferencias}
  u_{n+k} + \alpha_{k-1}u_{n+k-1}
  + \dots +
  \alpha_1 u_{n+1}+\alpha_0u_n
  =
  \phi_{n+k},
  \quad \alpha_0 \neq 0; n=0,1,\dots
\end{equation}
(dados $u_0,u_1,\dots,u_{k-1}$ ).
Si $\phi_{n+k}=0,n=0,1,\dots$, decimos que la ecuación en diferencias
es homogénea. Si $\alpha_{k-1},\alpha_{k-2},\dots,\alpha_0$ son
constantes, decimos que es una ecuación en diferencias con
coeficientes constantes.

Dados los valores iniciales $u_0,u_1,\dots,u_{k-1}$, se puede
determinar $u_n$ directamente a partir de la ecuación en diferencias,
sustituyendo sucesivamente.

Consideremos el problema homogéneo y busquemos soluciones de la forma
$u_n=r^{n}$. Entonces
\begin{equation}
  r^{n+k} + \alpha_{k-1}r^{n+k-1}
  + \dots +
  \alpha_1 r^{n+1}+\alpha_0r^n
  =
  0.
\end{equation}
Para $r=0$, obtenemos la solución trivial, así que supondremos que
$r\neq 0$, de modo que al cancelar $r^{n}$ obtenemos
\begin{equation}
  r^{k} + \alpha_{k-1}r^{k-1}
  + \dots +
  \alpha_1 r^{1}+\alpha_0
  =
  0.
\end{equation}
El lado izquierdo de esta ecuación se es un polinomio de grado $k$ y
se llama el polinomio característico de la ecuación en diferencias
\eqref{eq:ec_diferencias} 
Se denota con $\pi(r)$.
Si $\pi(r)$ tiene $k$ raíces distintas, digamos
$r_0,r_1,\dots,r_{k-1}$, entonces la solución general de la ecuación
en diferencias es
\begin{equation}\label{eq:sol_general}
  u_n = \gamma_0r_0^{n} + \dots + \gamma_{k-1}r_{k-1}^{n}
\end{equation}
y, dados $u_0,\dots,u_{k-1}$, existe un conjunto único de constantes
$\gamma_0,\dots,\gamma_{k_1}$ tal que \eqref{eq:sol_general}.

\begin{example}
  La sucesión de Fibonacci es la solución a la ecuación
  \begin{equation}
    u_{n+2} = u_{n+1} + u_n
  \end{equation}
  con condiciones iniciales $u_0=u_1=1$.
  Las soluciones de la forma $u_n=r^n$ satisfacen la ecuación
  característica
  \begin{equation}
    r^2 = r + 1
  .\end{equation}
  Es decir,
  \begin{equation}
    r = \frac{1\pm\sqrt 5}{2}
  .\end{equation}
  Por lo tanto,
  \begin{equation}
    u_n = \gamma_0
    \left( \frac{1+\sqrt 5}{2} \right)^n
    +
    \gamma_1
    \left( \frac{1-\sqrt 5}{2} \right)^n
  ,\end{equation}
  donde las constantes $\gamma_0$ y $\gamma_1$ se obtienen aplicando
  las condiciones iniciales
  \begin{align}
    1 = u_0 &= \gamma_0
    +
    \gamma_1
    \\
    1 = u_1 &= \gamma_0
    \left( \frac{1+\sqrt 5}{2} \right)
    +
    \gamma_1
    \left( \frac{1-\sqrt 5}{2} \right)
  ,\end{align}
  Resolviendo este sistema, obtenemos
  \begin{equation}
    \gamma_{0,1} = \frac{1}{2} \pm \frac{\sqrt 5}{10}
  .\end{equation}
\end{example}

Si la raíz $r_j$ tiene multiplicidad $m\geq 2$, debemos buscar
soluciones de la forma $p(n)r^n$, donde $p$ es un polinomio en $n$ de
grado $m-1$.

\begin{example}
  Consideremos la ecuación
  \begin{equation}
    u_{n+3} -2u_{n+2} - 7 u_{n+1} - 4u_n = 0
  \end{equation}
  con condiciones iniciales $u_0=1$, $u_1=0$, $u_{2}=-1$.
  La ecuación característica
  \begin{equation}
    r^{3}-2r^{2}-7r-4 = 0
  \end{equation}
  tiene una raíz simple $r_0=4$ y una raíz doble $r_{1,2}=-1$.
  Por lo tanto, la solución general es
  \begin{equation}
    u_n = \gamma_{0}4^n + (\gamma_1+\gamma_2n)(-1)^n
  .\end{equation}
  Aplicando las condiciones iniciales, obtenemos ecuaciones
  \begin{align}
    \gamma_0+\gamma_1 &= 1 \\
    4\gamma_0-\gamma_1-\gamma_2 &= 0 \\
    16\gamma_0+\gamma_1+2\gamma_2&= -1
  \end{align}
  con solución $\gamma_0=0$, $\gamma_1=1$, $\gamma_2=-1$.
  Por lo tanto, la solución es
  \begin{equation}
    u_n = (1-n)(-1)^{n}
  .\end{equation}
\end{example}

\section{Métodos multipaso}

\section{Métodos Adams}

El problema de valor inicial
\begin{equation}
  \left\{
    \begin{aligned}
      y'(t) &= f(t,y(t)) \\
      y(t_0) &= y_0
    \end{aligned}
  \right.
\end{equation}
en $I=[t_0,t_N]$, es equivalente a las ecuaciones integrales
\begin{equation}
  y(t_{n+1}) = y(t_n) + \int_{t_n}^{t_{n+1}}\! f(t,y(t))\,dt
\end{equation}
para $n=0,\dots,N-1$.

La aproximación se construye como
\begin{equation}
  u_{n+1} = u_n + \int_{t_n}^{t_{n+1}}\Pi(t)\,dt,
\end{equation}
donde $\Pi(t)$ es el polinomio interpolador de $f(t,y(t))$.


\subsection{Metodos Adams-Bashforth (explícitos)}
Interpolar en $t_{n},t_{n-1},\dots,t_{n-p}$.

\begin{example}
  Recordemos que el polinomio interpolador de Lagrange de grado $n$ 
  con datos $(x_0,y_0),(x_1,y_1),\dots,(x_n,y_n)$ se construye como
  \begin{equation}
    \Pi(x) =  L_0(x)y_0 + L_1(x)y_1 + \dots + L_n(x)y_n
  \end{equation}
  donde cada $L_i(x)$ es un polinomio de grado $n$ que satisface
  $L_i(x_j)=\delta_{ij}$. Explícitamente,
  \begin{equation}
    L_i(x) = \frac{\prod_{j\neq i}(x-x_j)}{\prod_{j\neq i}(x_i-x_j)}
  .\end{equation}
\end{example}

\begin{example}
  Interpolaremos $f(t,y(t))$ en $f_n,f_{n-1},f_{n-2}$. Entonces
  \begin{align}
    \Pi(t)
    &= \frac{(t-t_{n-1})(t-t_n)}{(-h)(-2h)}f_{n-2}
    +  \frac{(t-t_{n-2})(t-t_n)}{(h)(-h)}f_{n-1}
    +  \frac{(t-t_{n-2})(t-t_{n-1})}{(2h)(h)}f_{n}
    \\
    &=
    \frac{1}{2h^{2}}
    \left[
      (t-t_{n-1})(t-t_n)f_{n-2}
    - 2(t-t_{n-2})(t-t_n)f_{n-1}
    + (t-t_{n-2})(t-t_{n-1})f_{n}
    \right]
  .\end{align}
  Cualquier polinomio $f(t)$ de grado $2$ se puede integrar como
  \begin{equation}
    \int_{a}^{b}f(t)\,dt = \frac{b-a}{3}[f(a)+4f((a+b)/2)+f(b)]
  .\end{equation}
  Así,
  \begin{align}
     \int_{t_n}^{t_{n+1}}\Pi(t)\,dt
     =
     \frac{h}{6}[\Pi(t_{n})+4\Pi(t_{n}+h / 2)+\Pi(t_{n+1})]
  .\end{align}
\end{example}

\subsection{Métodos Adams-Moulton (implícitos)}
Interpolar con $t_{n+1},t_n,\dots,t_{n-p}$.

\begin{example}
  Supongamos que $\Pi(t)$ es el polinomio interpolador únicamente en
  un punto: $t_{n+1}$, es decir, $\Pi(t)=f_{n+1}$.
  Por lo tanto, el método
  \begin{equation}
    u_{n+1} = u_n + \int_{t_n}^{t_{n+1}} \Pi(t) \,dt
  \end{equation}
  se reduce a
  \begin{equation}
    u_{n+1} = u_n + hf_{n+1}.
  \end{equation}

  Para el caso de dos puntos, interpolaremos en $t_{n+1},t_n$:
   \begin{equation}
    \Pi(t)
    = \frac{t-t_{n+1}}{t_{n}-t_{n+1}}f_n
    + \frac{t-t_n}{t_{n+1}-t_n}f_{n+1}
  .\end{equation}
  Como $\Pi(t)$ es de grado $1$, podemos evaluar la integral con la
  regla trapezoidal (Simpson $\frac{1}{2}$) como sigue:
  \begin{align}
    u_{n+1}
    &= u_n + \int_{t_n}^{t_{n+1}} \Pi(t) \,dt \\
    &= u_{n} + \frac{h}{2}(\Pi(t_{n})+\Pi(t_{n+1})) \\
    &= u_{n} + \frac{h}{2}(f_{n}+f_{n+1}).
  \end{align}

  Para el caso de tres puntos, interpolaremos en $t_{n+1},t_n,t_{n-1}$ 
  como sigue:
  \begin{equation}
    \begin{split}
      \Pi(t)
      &=
      \frac{(t-t_{n})(t-t_{n+1})}{(t_{n-1}-t_n)(t_{n-1}-t_{n+1})}
      f_{n-1}
      +
      \frac{(t-t_{n-1})(t-t_{n+1})}{(t_{n}-t_{n-1})(t_{n}-t_{n+1})}
      f_{n}
      \\
      &\quad +
      \frac{(t-t_{n-1})(t-t_{n})}{(t_{n+1}-t_{n-1})(t_{n+1}-t_{n})}
      f_{n+1}
      \\
      &=
      \frac{(t-t_{n})(t-t_{n+1})}{(-h)(-2h)}
      f_{n-1}
      +
      \frac{(t-t_{n-1})(t-t_{n+1})}{(h)(-h)}
      f_{n}
      +
      \frac{(t-t_{n-1})(t-t_{n})}{(2h)(h)}
      f_{n+1}
      \\
      &=
      \frac{1}{2h^{2}}
      \left[
        (t-t_{n})(t-t_{n+1}) f_{n-1}
        - (t-t_{n-1})(t-t_{n+1}) f_{n}
        + (t-t_{n-1})(t-t_{n}) f_{n+1}
      \right]
    \end{split}
  \end{equation}
  Ahora $\Pi(t)$ es un polinomio de grado $2$, así que podemos
  evaluarlo con la regla de Simpson $\frac{1}{3}$ como sigue:
  \begin{align}
    u_{n+1}
    &= u_n + \int_{t_n}^{t_{n+1}} \Pi(t) \,dt \\
    &= u_{n}
    + \frac{h}{6}(\Pi(t_{n})+4\Pi(t_n + h / 2)+\Pi(t_{n+1})) \\
    &= u_{n}
    + \frac{h}{6}(f_{n}+4\Pi(t_n + h / 2)+f_{n+1}) \\
    &= 
  \end{align}
\end{example}

\subsection{Métodos BDF (backwards difference formulae)}

Dado el problema
\begin{equation}
  y'(t)=f(t,(t))
,\end{equation}
la idea es aproximar $y'(t_{n+1})$ con diferencias finitas atrasadas
de distintos órdenes (usando $t_{n+1},t_{n},t_{n-1},\dots,t_{n-p}$).

\begin{example}
  \begin{enumerate}
    \item
      Usando $t_{n+1},t_{n},t_{n-1}$, aproximar $y'(t_{n+1})$:
      \begin{equation}
        y'_{n+1} = ay_{n-1} + by_{n} + cy_{n+1}
      .\end{equation}
      Esto es
      \begin{equation}
        y'(t_{n}+h)
        = ay(t_{n}-h) + by(t_{n}) + cy(t_n+h)
      .\end{equation}
      Expandiendo en serie de Taylor hasta orden $2$, esto es
      \begin{align}
        &\hspace{-10mm}
        y'_n + hy''_n + \frac{h^{2}}{2}y'''_n + O(h^{3}) \\
        &= a \left[y_n - h y'_n + \frac{h^{2}}{2}y''_n\right]
        + b y_n
        + c \left[y_n + hy'_n + \frac{h^{2}}{2}y''_n\right]
        + O(h^{3})
      .\end{align}
      Luego, obtenemos el sistema de ecuaciones
      \begin{align}
        a+b+c &= 0, \\
        -ah+ch &= 1, \\
        \frac{ah^{2}}{2} + \frac{ch^{2}}{2} &= h,
      \end{align}
      que tiene solución
      \begin{align}
        a &= \frac{1}{2h}, &
        b &= -\frac{2}{h}, &
        c &= \frac{3}{2h}.
      \end{align}
      Aplicando la aproximación, esto es
      \begin{align}
        y'_{n+1}
        &= \frac{1}{2h}y_{n-1} - \frac{2}{h}y_n + \frac{3}{2h}y_{n+1}
        + O(h^{3})
        \\
        &= \frac{1}{2h}[y_{n-1} - 4y_n + 3y_{n+1}] + O(h^{3})
      .\end{align}
      Luego, obtenemos el método
      \begin{equation}
        f_{n+1} = \frac{1}{2h}[u_{n-1} - 4u_n + 3u_{n+1}]
      .\end{equation}
  \end{enumerate}
\end{example}o

\subsection{Análisis de los métodos multipaso}
El método multipaso
\begin{equation}
  u_{n+1}
  =
  \sum_{j=0}^{p}a_ju_{n-j}
  +
  h
  \sum_{j=0}^{p}b_jf_{n-j} + b_{-1}f_{n+1},
  \quad
  n=p, p+1,\dots
\end{equation}
tiene error de truncamiento local
\begin{equation}
  \tau_{n+1}
  =
  \frac{1}{h}
  \left[
    y_{n+1}
    -
    \sum_{j=0}^{p} a_jy_{n-j}
    +
    h
    \sum_{j=1}^{p}b_{j}y'_{n-j}
    +
    b_{-1}f_{n+1}
  \right]
,\end{equation}
donde $y_{n-j}=y(t_{n-j})$,
$y'_{n-j}=y'(t_{n-j})=f(t_{n-j},y(t_{n-j}))$.
El error de truncamiento global es
\begin{equation}
  \tau(h) = \max_n |\tau_{n}(h)|
.\end{equation}

El método es consistente si $\lim_{h\to 0}\tau(h)=0$.
El método tiene orden $q$ si $\tau(h)=O(h^{q})$ para algún $q\geq 1$.
Si el método tiene orden $q$, entonces
\begin{equation}
  \tau_{n+1}(h) = C_qh^qy^{(q+1)}(t_n) + O(h^{q+1})
.\end{equation}
La constante $C_q$ se llama constante de error.

\begin{example}
  Método de Euler:
  \begin{equation}
    u_{n+1} = u_{n} + hf_n
  .\end{equation}
  Entonces
  \begin{align}
    \tau_{n+1}
    &= \frac{1}{h}(y_{n+1} - (y_n+hy'_n)) \\
    &= \frac{1}{h}
    \left(
    y_n + hy'_n + \frac{h^{2}}{2} y''_n + O(h^{3})
    - (y_n + hy'_n))
    \right)
    \\
    &= \frac{h}{2}y''_n + O(h^{3})
  .\end{align}
  Así, el método de Euler es de orden $1$ y la constante de error es
  $C_1=\frac{1}{2}$.
\end{example}

\begin{theorem}
  El método multipaso es consistente si se cumplen las condiciones
  \begin{equation}\label{eq:consistencia_multi}
    \sum_{j=0}^{p}a_j = 1,
    \qquad
    -\sum_{j=0}^{p}ja_j + \sum_{j=-1}^{p}b_j = 1.
  \end{equation}
  Más aún, si $y\in C^{q+1}(I)$ para algún $q\geq 1$, entonces el
  método es de orden $q$ si, y solo si, se satisfacen
  \eqref{eq:consistencia_multi} y
  \begin{equation}
    \sum_{j=0}^{p}(-j)^{i}a_j + i \sum_{j=-1}^{p}(-j)^{i-1}b_j = 1,
    \quad i=2,3,\dots,q
  .\end{equation}
\end{theorem}
\begin{proof}
  (Solo para la parte de consistencia).

  Primero, observemos que
  \begin{equation}
    y_{n-j} = y_{n} - jhy'_n + O(h^{2}),
    \qquad 
    y'_{n-j} = y'_n + O(h)
  .\end{equation}
  Así,
  \begin{align}
    h\tau_{n+1}
    &= y_{n+1}
    - \sum_{j=0}^{p}a_jy_{n-j}
    - h \sum_{j=-1}^{p}b_{j}y'_{n-j} \\
    &= y_{n+1}
    - \sum_{j=0}^{p}a_j(y_n - jhy'_n + O(h^{2}))
    - h \sum_{j=-1}^{p}b_{j}(y'_{n}+O(h)) \\
    &= y_{n+1}
    - y_n \sum_{j=0}^{p}a_j
    + hy'_n \sum_{j=0}^{p}ja_j
    - hy'_n \sum_{j=-1}^{p}b_{j} + O(h^{2})
    \\
    &= y_{n+1} - y_n
    + y_n\left(1-\sum_{j=0}^{p}a_j\right)
    - hy'_n
    \left( - \sum_{j=0}^{p}ja_j + \sum_{j=-1}^{p}b_{j} \right)
    + O(h^{2})
  .\end{align}
  
  Si se cumplen las hipotesis del teorema, el primer
  paréntesis es $0$ y el segundo es $1$, de modo que
  \begin{equation}
    \tau_{n+1}
    = \frac{y_{n+1} - y_n}{h} - y'_n + O(h)
    = \frac{y_{n+1} - y_n}{h} - y'_n + O(h)
    \to 0
  \end{equation}
  cuando $h\to 0$.
\end{proof}

\subsection{Condiciones de la raíz}

El problema de prueba
\begin{equation}
  \left\{
    \begin{aligned}
      y' &= \lambda y, \\
      y(0) &= 1.
    \end{aligned}
  \right.
\end{equation}
tiene $f(t,y)=\lambda y$. Aplicando el método multipaso, obtenemos
\begin{equation}
  u_{n+1}
  =
  \sum_{j=0}^{p}a_ju_{n-j}
  +
  h\lambda
  \sum_{j=-1}^{p}b_ju_{n-j}
  \quad
  n=p, p+1,\dots
\end{equation}
la cual es una ecuación en diferencias con coeficientes constantes.
Su polinomio característico
\begin{equation}
  \pi(r)
  = 
  r^{n+1}
  -
  \sum_{j=0}^{p}a_jr^{n-j}
  -
  h\lambda
  \sum_{j=-1}^{p}b_jr^{n-j}
\end{equation}
se puede expresar en la forma
\begin{equation}
  \pi(r) = p(r) - h\lambda\sigma(r)
\end{equation}
donde
\begin{equation}
  \rho(r)
  =
  r^{p+1}
  -
  \sum_{j=0}^{p}a_jr^{p-j},
  \qquad
  \sigma(r)
  =
  \sum_{j=-1}^{p}b_jr^{p-j}
.\end{equation}

Si $r_i(h\lambda)$ son las raíces de $\pi(r;h\lambda)$, entonces las
soluciones fundamentales son de la forma $u_n = [r_i(h\lambda)]^n$.

\begin{definition}[Condición absoluta de la raíz]
  El método satisface la \emph{condición absoluta de la raíz} si
  existe $h_0>0$ tal que, para $h\in (0,h_0]$, entonces
  \begin{equation}
    |r_j(h\lambda)|<1, \quad j=0,1,\dots,p
  .\end{equation}
\end{definition}

\begin{definition}[Condición de la raíz]
  Sean $r_j=r_j(0)$ las raíces de $\rho(r)$.
  El método multipaso satisface la \emph{condición de la raíz} si
  todas las
  raíces $r_i$ están contenidas en el disco unitario (cerrado) con
  centro en el origen y, si estas caen en la frontera, entonces
  deberán ser simples. I.e.
  \begin{equation}
    |r_j|\leq 1, \quad j=0,\dots,p
    \quad \text{y} \quad
    \text{si $|r_j|=1$, entonces } \rho'(r_j)\neq 0
  .\end{equation}
\end{definition}

\begin{definition}[Condición fuerte de la raíz]
  El método multipaso satisface la \emph{condición fuerte de la raíz}
  si satisface la condición de la raíz y $r_0=1$ es la única raíz que
  está en la frontera del disco unitario.
\end{definition}

\begin{definition}[Cero-estabilidad para métodos multipaso]
  Consideremos un método multipaso y una perturbación por
  $\delta_{n}$
  \begin{align}
    &\left\{
      \begin{aligned}
        u_{n+1}
        &= \sum_{j=0}^{p}a_ju_{n-j}
        + h \sum_{j=-1}^{p} b_jf_{n-j} \\
        u_k &= w_k, \quad k=0,1,\dots,p
      \end{aligned}
    \right.
    \label{eq:metodosinperturbar}
    \\
    &\left\{
      \begin{aligned}
        z_{n+1}
        &= \sum_{j=0}^{p}a_jz_{n-j}
        + h \sum_{j=-1}^{p} b_jf(t_{n-j},z_{n-j}) + h\delta_{n+1} \\
        z_k &= w_k + \delta_k, \quad k=0,1,\dots,p
      \end{aligned}
    \right.
  .\end{align}
  Decimos que el método \eqref{eq:metodosinperturbar}  
  es cero-estable si existen $h_0,C>0$ tales
  que, para $h\in(0,h_0]$
  \begin{equation}
    |z_n-u_n|\leq C\epsilon, \quad 0\leq n\leq N_h
  \end{equation}
  siempre que $|\delta_n|<\epsilon, 0\leq n\leq N_h$.
\end{definition}

\begin{theorem}[Equivalencia de estabilidad cero y la condición de la
  raíz]\label{thm:estabilidad_y_raiz}
  Para un método multipaso consistente, la condición de la raíz es
  equivalente a la estabilidad cero.
\end{theorem}

Con esta equivalencia se puede probar el siguiente resultado de
convergencia.

\begin{theorem}[Condición de la raíz implica convergencia]
  \label{thm:raiz_y_convergencia}
  Un método multipaso consistente es convergente si, y solo si,
  satisface la condición de la raíz y el error en los valores
  iniciales tiende a cero conforme $h$ tiende a cero. Más aún, el
  método converge con orden $q$ si $\tau(h)=O(h^q)$ y el error en los
  datos iniciales tiende a cero como $O(h^q)$.
\end{theorem}

\begin{example}
  Consideremos el método de Euler
  \begin{equation}
    u_{n+1} = u_n + hf_n
  \end{equation}
  para el problema lineal $y'(t)=f(t,y(t))=\lambda y(t)$.
  Tenemos
  \begin{equation}
    u_{n+1} = u_n + h\lambda u_n
  .\end{equation}
  La ecuación característica es
  \begin{equation}
    r - 1 - h\lambda = 0
  .\end{equation}
  Esta se puede escribir como
  \begin{equation}
    \pi(r;h\lambda) = \rho(r) - h\lambda\sigma(r) = 0
  \end{equation}
  con $\rho(r)=r-1$ y $\sigma(r)=1$.
  Aquí, $\rho(r)$ solo tiene raíz simple $r_0=1$. Como esta raíz está
  contenida en el disco unitario cerrado, el método cumple la
  condición de la raíz. Por los teoremas
  \ref{thm:estabilidad_y_raiz} y \ref{thm:raiz_y_convergencia}, 
  el método es convergente.
\end{example}

\begin{example}[Métodos Adams]
  Consideremos el método
  \begin{equation}
    u_{n+1} = u_n + h \sum_{j=1}^{p}b_jf_{n-j}
  .\end{equation}
  Aplicándolo al problema lineal $y'(t)=f(t,y(t))=\lambda y(t)$,
  tenemos $f_{n-j}=\lambda u_{n-j}$, tenemos
  \begin{equation}
    u_{n+1} - u_n - \lambda h \sum_{j=1}^{p}b_ju_{n-j} = 0
  .\end{equation}
  Para encontrar el polinomio característico, sustituyamos
  $u_n=r^n$:
  \begin{equation}
    r^{n+1} - r^n - \lambda h \sum_{j=1}^{p}b_jr^{n-j} = 0
  .\end{equation}
  La menor potencia de $r$ que aparece en la suma es $r^{n-p}$,
  la cual es $1$ para $n=p$:
  \begin{equation}
    r^{p+1} - r^p - \lambda h \sum_{j=1}^{p}b_jr^{p-j} = 0
  .\end{equation}
  Poniendo $i=p-j$, tenemos $j=p-i$, donde $i$ va desde $0$ hasta
  $p-1$:
  \begin{equation}
    r^{p+1} - r^p - \lambda h \sum_{i=0}^{p-1}b_{p-i}r^{i} = 0
  .\end{equation}
  Este es el polinomio característico buscado. Se puede escribir como
  \begin{equation}
    \pi(r,h\lambda) = \rho(r) - h\lambda\sigma(r)
  \end{equation}
  donde
  \begin{align}
    \rho(r) &= r^{p+1}-r^p = r^p(r-1) \\
    \sigma(r) &= \sum_{i=0}^{p-1}b_{p-i}r^{i}
  .\end{align}
  Las raíces de $\rho(r)$ son $0$ (con multiplicidad $p$) y $1$ (raíz
  simple). Como todas las raíces están en el disco unitario cerrado y
  no hay raíces múltiples en la frontera, se cumple la condición de la
  raíz y, por los teoremas, se sigue que los métodos son convergentes.
\end{example}

\subsection{Estabilidad absoluta}
Al aplicar el método multipaso al problema de prueba $y'=\lambda y$,
$y(0)=1$, los métodos multipaso se reducen a una ecuación en
diferencias con soluciones de la forma
\begin{equation}
  u_n = \sum_{j=1}^{k'}
  \left(
    \sum_{s=0}^{m_j-1} \gamma_{s_j} n^{s}
  \right)
  r_{j}(h\lambda)^n
,\end{equation}
donde $k'$ es el número de raíces distintas de $\pi(r;h\lambda)$ y
$m_j$ es la multiplicidad de $r_j(h\lambda)$.

Al igual que en los métodos de un paso,
\begin{equation}
  A 
  = \{
    z=h\lambda
  \mid
  |u_n|\to 0 \text{ cuando } n\to \infty,
  \Re(\lambda)<0
\}
.\end{equation}

\begin{remark}
  \begin{enumerate}
    \item
      Primera barrera de Dahlquist.
      No existe método multipaso de $p$ pasos que sea cero estable con
      orden mayor a $p+1$, si $p$ es impar, y mayor a $p+2$, si $p$ es
      par.
    \item
      Segunda barrera de Dahlquist. Un método multipaso lineal
      explícito no puede ser incondicionalmente estable ni
      $\nu$-estable. Más aún, no existe método multipaso lineal que
      sea incondicionalmente estable de orden mayor a $2$.
    \item
      Finalmente, para cualquier $\nu\in(0,\pi / 2)$ solo existen
      métodos multipasos de $p$ pasos $\nu$-estables de orden $p$ para
      $p=3$ y $p=4$.
  \end{enumerate}
\end{remark}

\subsection{Métodos predictor - corrector}

\begin{example}
  Consideremos el método trapezoidal
  \begin{equation}
    u_{n+1} = u_n + \frac{h}{2}(f_n + f_{n+1})
  \end{equation}
  con $u_0=y_0$.

  Para calcular $u_1$, necesitamos resolver la ecuación
  \begin{equation}
    u_{n+1} = u_n + \frac{h}{2}(f_n + f(t_{n+1},u_{n+1})).
  \end{equation}
\end{example}

La idea es
\begin{itemize}
  \item
    Comenzar con $\tilde p + 1$ datos iniciales
    \begin{equation}
      u_0,u_1,\dots,u_{\tilde p}
    \end{equation}
    y a partir de estos generar los $\tilde p+1$ valores iniciales de
    $f$:
    \begin{align}
      f_0 &= f(t_0,u_0) \\
      f_1 &= f(t_1,u_1) \\
      \vdots \\
      f_{\tilde p} &= f(t_{\tilde p},u_{\tilde p})
    .\end{align}
  \item
    Con esta información inicial, primero hacer una estimación de
    $u_{n+1}$, digamos $u_{n+1}^{(0)}$, usando un método explícto de
    $\tilde p+1$ pasos (con $\tilde p+1$ coeficientes $\tilde a_j,
    \tilde b_j$, $0\leq j\leq \tilde p$).
    %\begin{align}
    %  u^{(m)}_0 &= u_0, \\
    %  u^{(m)}_1 &= u_1, \\
    %  \vdots \\
    %  u^{(m)}_{\tilde p} &= u_{\tilde p}
    %,\end{align}
    %podemos calcular
    %\begin{align}
    %  f^{(m-1)}_0 &= f(t_0,u_0) = f_0, \\
    %  f^{(m-1)}_1 &= f(t_1,u_1) = f_1, \\
    %  \vdots \\
    %  f^{(m-1)}_{\tilde p} &= f(t_{\tilde p},u_{\tilde p})
    %  = f_{\tilde p}
    %,\end{align}
    \begin{align}
      u_{\tilde p+1}^{(0)}
      &=
      \tilde a_{0} u_{\tilde p}
      + \tilde a_{1} u_{\tilde p - 1}
      + \dots +
      \tilde a_{\tilde p} u_{0}
      + h
        [
        \tilde b_0 f_{\tilde p}
        +
        \tilde b_1 f_{\tilde p-1}
        + \dots +
        \tilde b_{\tilde p} f_{0}
        ]
      \\
      &=
      \sum_{j=0}^{\tilde p}\tilde a_j u_{\tilde p-j}
      + h \sum_{j=0}^{\tilde p}\tilde b_j f_{\tilde p-j}
    .\end{align}
  \item
    Luego, aplicamos $m$ ciclos de evaluar $f$ y luego corregir con un
    método implícito de $p+1$ pasos, donde  $p\leq\tilde p$.
    \begin{align}
      f_{\tilde p + 1}^{(0)}
      &= f(t_{\tilde p+1},u_{\tilde p + 1}^{(0)}),
      &
        u_{\tilde p + 1}^{(1)}
        &= \sum_{j=0}^{p}a_ju_{\tilde p-j}
        + h b_{-1}f_{\tilde p+1}^{(0)}
        + h \sum_{j=0}^{p}b_jf_{\tilde p-j},
        \\
      f_{\tilde p + 1}^{(1)}
      &= f(t_{\tilde p+1},u_{\tilde p + 1}^{(1)}),
      &
        u_{\tilde p + 1}^{(2)}
        &= \sum_{j=0}^{p}a_ju_{\tilde p-j}
        + h b_{-1}f_{\tilde p+1}^{(1)}
        + h \sum_{j=0}^{p}b_jf_{\tilde p-j},
        \\
      \vdots \nonumber
      \\
      f_{\tilde p + 1}^{(m-1)}
      &= f(t_{\tilde p+1},u_{\tilde p + 1}^{(m-1)}),
      &
        u_{\tilde p + 1}^{(m)}
        &= \sum_{j=0}^{p}a_ju_{\tilde p-j}
        + h b_{-1}f_{\tilde p+1}^{(m-1)}
        + h \sum_{j=0}^{p}b_jf_{\tilde p-j}.
    \end{align}
    Tomamos $u_{\tilde p+1}$ como el resultado de la $m$-ésima
    iteración. Es decir, $u_{\tilde p+1} = u_{\tilde p + 1}^{(m)}$.
  \item
    Estos últimos dos pasos son el primer paso del siguiente ciclo:
    Para cada $\tilde p\leq n\leq N-1$, se hace
    \begin{align}
      u_{n+1}^{(0)}
      &=
      \sum_{j=0}^{n}\tilde a_j u_{n-j}
      + h \sum_{j=0}^{n}\tilde b_j f_{n-j}
    ,\end{align}
    seguido de las $m$ iteraciones siguientes, donde $0\leq k\leq
    m-1$:
    \begin{align}
      f_{n + 1}^{(k)}
      &= f(t_{n+1},u_{n + 1}^{(k)}),
      &
        u_{n + 1}^{(k+1)}
        &= \sum_{j=0}^{p}a_ju_{n-j}
        + h b_{-1}f_{n+1}^{(k-1)}
        + h \sum_{j=0}^{p}b_jf_{n-j},
    \end{align}
    y luego se define $u_{n+1}=u_{n+1}^{(m)}$.
\end{itemize}
  

\subsection{Metodos Runge-Kutta}
Nota: a partir de la serie de Taylor
\begin{equation}
  y(t+h) = y(t) + hy'(t) + \frac{h^{2}}{2}y''(t) + \dots
.\end{equation}
se pueden obtener métodos ``de Taylor''
\begin{align}
  u_{n+1} &= u_n + hf_n \\
  u_{n+1} &= u_n
  + hf_n
  + \frac{h^{2}}{2}(\partial_1f_n + f_n \partial_2 f_n)
.\end{align}
Por construcción, estos métodos son de orden $1$ y orden $2$,
respectivamente. La idea de los métodos Runge-Kutta es mantener esta
propiedad sin tener que calcular las derivadas de $f$ directamente.
Escribamos la expansión anterior en la forma
\begin{align}\label{eq:orig}
  y(t+h)
  &= y(t)
    + h f(t,y)
    + \frac{h^{2}}{2}
    \left[ (\partial_1 f + f \partial_2 f)|_{(t,y)} \right]
    + O(h^{3}) \\
  &= y(t)
    + h f(t,y)
    + \frac{h}{2}
    \left[ (h\partial_1 f + h f \partial_2 f)|_{(t,y)} \right]
    + O(h^{3})
.\end{align}
Ahora queremos comparar esto con una expansión en serie de potencias
de dos variables:
\begin{align}
  f(t+h,y+k)
  &=
  f(t,y) + h \partial _1 f + k \partial _2 f + O(h^{2},hk,k^{2})
  \\
  &=
  f(t,y) + h \partial _1 f + k \partial _2 f + O(h^{2},hk,k^{2}).
\end{align}
Poniendo $k=hf$, tenemos
\begin{align}
  f(t+h,y+hf)
  &=
  f(t,y) + h \partial _1 f + hf \partial _2 f
  + O(h^{2}),
\end{align}
i.e.
\begin{align}
  h \partial _1 f + hf \partial _2 f
  &=
  f(t+h,y+hf) - f(t,y)
  + O(h^{2},hk,k^{2}).
\end{align}
Sustituyendo en \eqref{eq:orig}, obtenemos
\begin{align}\label{eq:sust}
  y(t+h)
  &= y(t)
    + hf(t,y(t))
    + \frac{h}{2}
    \left[
    f(t+h,y+hf) - f(t,y)
    \right]
    + O(h^{3})
  \\
  &= y(t)
    + \frac{h}{2} f(t,y(t))
    + \frac{h}{2} f(t+h,y+hf)
    + O(h^{3})
.\end{align}
Esto nos sugiere el método
\begin{equation}
  y_{n+1} = y_n + \frac{h}{2} \left[
    f_n + f(t_n + h, y_n + hf_n)
  \right]
,\end{equation}
que reconocemos como el método de Heun.

En general, los métodos Runge-Kutta son métodos de un paso
\begin{equation}
  u_{n+1} = u_n + h F(t_n,u_n,h,f)
\end{equation}
donde $F$ es una función que aproxima la pendiente como una
combinación lineal
\begin{align}
  F
    &=
    \sum_{i=1}^{s}b_ik_i
  \\[-3mm]
  \text{donde} \quad
  k_i
    &= f(t_n+c_ih, u_n+h \sum_{j=1}^{s}a_{ij} k_j)
    \\[-4mm]
  \text{y} \quad
  c_i
    &= \sum_{j=1}^{s}a_{ij}
.\end{align}
El método Runge-Kutta  se puede identificar por medio del
\emph{tablero de Butcher}
\begin{equation}
  \begin{array}{c|c}
    c & A \\[3mm]
    \hline \\[-2mm]
      & b^{T}
  \end{array}
,\end{equation}
donde $A$, $b$ y $c$ son los coeficientes $a_{ij}$, $b_i$, $c_i$.

\begin{remark}
  El método es
  \begin{itemize}
    \item
      explícito si $A$ es estrictamente triangular inferior
      ($a_{ij}=0$ siempre que $j\geq i$),
    \item
      semi-implícito si $A$ es triangular inferior ($a_{ij}=0$ para
      $j>i$),
    \item
      implícito si $A$ es ``llena''.
  \end{itemize}
\end{remark}

\subsection{Estabilidad absoluta de los métodos Runge-Kutta}

\begin{example}
  El método de Heun es
  \begin{align}
    u_{n+1} &= u_n
    + \left[ \frac{1}{2} K_1 + \frac{1}{2} K_2 \right], \\
    K_1 &= f(t_n,u_n), \\
    K_2 &= f(t_h+h,u_n+hK_1)
  .\end{align}
  En otras palabras:
  \begin{align}
    u_{n+1} &= u_n
    + \left[ \frac{1}{2} K_1 + \frac{1}{2} K_2 \right], \\
    K_1 &= f(t_n + 0 h,u_n + h(0 K_1 + 0 K_2)), \\
    K_2 &= f(t_h + 1 h,u_n+h(1 K_1 + 0 K_2)
  .\end{align}
  Por lo tanto, el tablero de Butcher del método es
  \begin{equation}
    \begin{array}{c|c}
      c & A \\[3mm]
      \hline \\[-2mm]
        & b^{T}
    \end{array}
    =
    \begin{array}{c|cc}
      0 & 0 & 0 \\[3mm]
      1 & 1 & 0 \\[3mm]
      \hline \\[-2mm]
        & \frac{1}{2} & \frac{1}{2}
    \end{array}
  .\end{equation}
  Así, la función de estabilidad es
  \begin{align}
    R(z)
    &= \det(I - zA + z\ol{1}b^{T}) \\
    &=
    \det
    \begin{bmatrix}
      1 + \frac{z}{2} & \frac{z}{2} \\
      -\frac{z}{2} & 1 + \frac{z}{2}
    \end{bmatrix}
    \\
    &= 1 + z + \frac{z^{2}}{2}
  \end{align}
  y la región de estabilidad absoluta es la que cumple $|R(z)|<1$,
  como ya habíamos determinado antes.
\end{example}

\subsection{Tamaño de paso adaptativo}

Existen varios métodos. Los más comunes son
\begin{itemize}
  \item
    Métodos basados en usar un mismo método con tamaño de paso $h$ y
    $\frac{h}{2}$ para combinarlas y obtener una mejor aproximación y
    una estimación del error (extrapolación de Richardson).
    Generalmente son más costosos que otras estrategias.
  \item
    Utilizar un par de métodos Runge-Kutta de órdenes $p,p+1$ y de
    $s,s+1$ etapas, respectivamente, que tengan en común las primeras
    $s$ filas de $A$. (La idea es que los mismos cálculos se puedan
    usar para los dos métodos). Se usan los resultados para estimar el
    error. (Métodos de Runge-Kutta Fehlberg).
\end{itemize}

\subsubsection{Método Runge-Kutta Fehlberg}
Consideremos dos métodos
\begin{align}
  u_{n+1}
    &= u_n + h\Phi(t_n,u_n,f_n;h)
    \qquad
    \text{(de orden $p$)} \\
  \tilde u_{n+1}
    &= \tilde u_n
    + h\tilde \Phi(t_n,\tilde u_n,\tilde f_n;h)
    \qquad
    \text{(de orden $p+1$)}
\end{align}
Es decir, tenemos
\begin{align}
  y_{n+1}
  &=
  y_n
  +
  h
  \Phi(t_n,y_n,f(t_n,y_n);h)
  + h\tau_{n+1}(h)
  \\
  y_{n+1}
  &=
  y_n
  +
  h
  \tilde \Phi(t_n,y_n,f(t_n,y_n);h)
  + h\tilde\tau_{n+1}(h)
,\end{align}
donde $\tau_{n+1}(h) = O(h^p)$ y $\tilde \tau_{n+1}(h) = O(h^{p+1})$.
Supongamos que la aproximación en $n$-ésimo paso es suficientemente
buena: $u_n\approx\tilde u_{n}\approx y_n$. Entonces
\begin{align}
  y_{n+1}
    &\approx
    u_n
    + h\Phi(t_n,u_n,f(t_n,u_n);h)
    + h\tau_{n+1}(h)
  \\
  y_{n+1}
    &\approx
    \tilde u_n
    + h\tilde \Phi(t_n,\tilde u_n,\tilde f(t_n,\tilde u_n);h)
    + h \tilde\tau_{n+1}(h)
\end{align}
i.e.
\begin{align}
  y_{n+1}
    &\approx
    u_{n+1}
    + h\tau_{n+1}(h)
  \\
  y_{n+1}
    &\approx
    \tilde u_{n+1}
    + h\tilde\tau_{n+1}(h)
.\end{align}
En particular, de la primera de estas aproximaciones
tenemos $e_{n+1} = y_{n+1} - u_{n+1} \approx h \tau_{n+1}(h)$. Así,
\begin{align}
  u_{n+1} - \tilde u_{n+1}
  &\approx (y_{n+1} - h\tau_{n+1}(h)) - (y_{n+1} -
  h\tilde\tau_{n+1}(h))
  \\
  &= - h\tau_{n+1}(h) + h\tilde\tau_{n+1}(h) \\
  &= - h\tau_{n+1}(h) + O(h^{p+2}).
\end{align}
Notemos que, entonces
\begin{align}
  |u_{n+1} - \tilde u_{n+1}|
  &= h|\tau_{n+1}(h)| \\
  &= |y_{n+1} - u_{n+1}| \\
  &= |e_{n+1}|.
\end{align}

\subsection{Problema de tarea}

Los métodos de Runge-Kutta son
\begin{equation}
  u_{n+1} = u_n + h F(t_n,u_n,h,f)
\end{equation}
\begin{align}
  F
    &=
    \sum_{i=1}^{s}b_ik_i
  \\[-3mm]
  \text{donde} \quad
  k_i
    &= f(t_n+c_ih, u_n+h \sum_{j=1}^{s}a_{ij} k_j)
    \\[-4mm]
  \text{y} \quad
  c_i
    &= \sum_{j=1}^{s}a_{ij}
.\end{align}

Aplicado al problema de prueba $f(t,y)=\lambda y$, tenemos
\begin{align}
  k_i
  &= \lambda(u_n+h \sum_{j=1}^{s}a_{ij} k_j) \\
  &= \lambda u_n+z \sum_{j=1}^{s}a_{ij} k_j
,\end{align}
así que, poniendo los $k_i$ en un vector columna $K$, esto es
\begin{align}
  K = \lambda u_n\bar 1+zAK
.\end{align}
Despejando $K$, esto es
\begin{equation}
  K=\lambda (I-zA)^{-1}u_n\bar 1
.\end{equation}

\chapter{Problemas de valores en la frontera (elípticas)}

El problema
\begin{equation}
  \left\{
    \begin{aligned}
      -u'' &= f, \\
      u(0) &= 0, \\
      u(1) &= 0,
    \end{aligned}
  \right.
\end{equation}
tiene solución general dada por
\begin{equation}
  u(x) = \int_{0}^{1}G(x,s)f(s)\,ds,
\end{equation}
donde $G(x,s)$ es la función de Green para este problema.
Explícitamente, se puede calcular que
\begin{equation}
  G(x,s)
  =
  \begin{cases}
    s(1-x), & 0\leq s\leq x, \\
    x(1-s), & x\leq s\leq 1.
  \end{cases}
\end{equation}

A partir de esta expresión para $u(x)$ podemos concluir que
\begin{itemize}
  \item
    si $f\in C^{m}[0,1]$, entonces $u\in C^{m+2}[0,1]$,
  \item
    si $f\geq 0$ en $[0,1]$, entonces $u\geq 0$ en $[0,1]$,
  \item
    $\|u\|_{\infty}=\frac{1}{8}\|f\|_\infty$.
\end{itemize}

\section{Diferencias finitas en dimensión 1}
Consideremos el problema en el intervalo $[0,1]$. Aproximaremos la
solución en los puntos equidistantes $x_0,x_1,\dots,x_n,x_{n+1}$,
donde $x_0=0$ y $x_{n+1}=1$. Es decir, $x_j=jh$ para
$j=0,1,\dots,n,n+1$, donde $h=\frac{1}{n+1}$.
Denotaremos como $u$ a la función a aproximar. Las aproximaciones en
los $x_j$ se denotarán como $u_j$. Entonces la versión discreta de
$-u''=f$ es
\begin{equation}
  - \left( \frac{u_{j-1}-2u_j+u_{j+1}}{h^{2}} \right)
    =
    f(x_j), \quad j=1,\dots,n
.\end{equation}
Es decir,
\begin{equation}
  - u_{j-1}+2u_j-u_{j+1}
    =
    h^{2} f(x_j), \quad j=1,\dots,n
.\end{equation}

Sin consideramos las condiciones de frontera más generales
\begin{equation}
  u(a)=\alpha
  \qquad u(b)=\beta
  \qquad x_j = a+jh, \quad j=0,1,\dots,n,n+1,
  \text{ donde } h = \frac{b-a}{n+1}
.\end{equation}
En este caso, tenemos $u_0=\alpha$, $u_{n+1}=\beta$ y obtenemos las
ecuaciones
\begin{align}
  -u_0+2u_1-u_2 &= h^{2}f(x_1) \\
  -u_1+2u_2-u_3 &= h^{2}f(x_2) \\
                &\vdots \nonumber \\
  -u_{n-1}+2u_n-u_{n+1} &= h^{2}f(x_n)
.\end{align}
Este es un sistema de ecuaciones
\begin{equation}
  \begin{bmatrix}
    2 & -1 \\
    -1 & 2 & -1 \\
       & -1 & 2 & -1 \\
       & & -1 & 2 & -1 \\
       &&&\ddots &\ddots &\ddots \\
       &&& & -1 & 2 & -1 \\
       &&&& & -1 & 2
  \end{bmatrix}
  \begin{bmatrix}
    u_1 \\ u_2 \\ u_3 \\ u_4 \\ \vdots \\ u_{n-2} \\ u_{n-1} \\ u_n
  \end{bmatrix}
  =
  h^{2}
  \begin{bmatrix}
    f(x_1) \\ f(x_2) \\ f(x_3) \\ f(x_4) \\
    \vdots \\ f(x_{n-1}) \\ f(x_n)
  \end{bmatrix}
  +
  \begin{bmatrix}
    \alpha \\ 0 \\ 0 \\ 0 \\ \vdots \\ 0 \\ \beta
  \end{bmatrix}
.\end{equation}
El problema tiene solución única, ya que la matríz de la izquierda es
simétrica y definida positiva. Consideraciones acerca de qué metodos
son adecuados para resolverlo (e.g., factorización de Cholesky)
se pueden encontrar en el libro de álgebra lineal de James Demmel.


\subsection{Consistencia}

Sea $L_h$ el operador dado como
\begin{equation}
  (L_hw)(x_j)
  =
  -\frac{w_{j-1}-2w_j+w_{j+1}}{h^{2}}, \quad j=1,\dots,n
.\end{equation}
Consideramos el dominio de $L_h$ como
\begin{equation}
  V_{h}^{\circ} =
  \left\{
    w=(w_0,w_1,\dots,w_n,w_{n+1}) \mid w_0=w_{n+1}=0
  \right\}
.\end{equation}
También podemos considerar las $w\in V_h^{\circ}$ como funciones
$\{x_0,x_1,\dots,x_n,x_{n+1}\}\to\R$.
El operador $L_h$ es la versión discreta del operador $-\partial^{2}$
en $C^{2}[0,1]$. Partiendo de la ecuación
\begin{equation}
  -u''(x) = f(x)
,\end{equation}
definimos el error de truncamiento local como la diferencia
\begin{equation}
  (L_h u)(x_j) = f(x_j) + \tau_h(x_j), \quad j=1,\dots,n
.\end{equation}
Para un punto interior ($j=1,\dots,n$), el operador $L_h$ es
\begin{align}
  (L_hu)(x_j)
  &= -\frac{1}{h^{2}}[u(x_{j-1})-2u(x_j)+u(x_{j+1})] \\
  &= -\frac{1}{h^{2}}
  \Big[
    u(x_j)
    - hu'(x_j)
    + \frac{h^{2}}{2}u''(x_j)
    - \frac{h^{3}}{3!} u'''(x_j)
    + \frac{h^{4}}{4!}u''''(\xi_i)  \\
  &\hspace{10mm} - 2u(x_j) \\
  &\hspace{10mm}
    + u(x_j)
    + hu'(x_j)
    + \frac{h^{2}}{2}u''(x_j)
    + \frac{h^{3}}{3!} u'''(x_j)
    + \frac{h^{4}}{4!}u''''(\eta_i)
  \Big]
  \\
  &= -\frac{1}{h^{2}}
  \left[
    h^{2}u''(x_j)
    + \frac{h^{4}}{4!}u''''(\xi_i)
    + \frac{h^{4}}{4!}u''''(\eta_i)
  \right] \\
  &=
  -
    u''(x_j)
  -
    \frac{h^{2}}{4!} [ u''''(\xi_i) + u''''(\eta_i) ]
,\end{align}
donde $\xi_i\in(x_{j-1},x_{j})$ y $\eta_j\in(x_{j},x_{j+1})$.
Si $u$ es la solución exacta a la ecuación $-u''=f$, entonces también
tenemos $u''''=-f''$, así que
\begin{equation}
  (L_hu)(x_j)
  =
  f(x_j) + \frac{h^{2}}{4!} [ f''(\xi_i) + f''(\eta_i) ]
,\end{equation}
por lo cual
\begin{align}
  \tau_{h}(x_j)
  &= (L_hu)(x_j) - f(x_j) \\
  &=
    \frac{h^{2}}{4!} [ f''(\xi_i) + f''(\eta_i) ]
.\end{align}
Si $\|f''\|_{\infty}\leq M$, entonces cada una de estos términos está
acotado por $h^{2}M/12$, así que podemos acotar el error de
truncamiento (global) como
\begin{align}
  \tau_{h}
  &= \max_{j=1,\dots,n} |\tau_{h}(x_j)| \\
  &\leq \frac{h^{2}M}{12}
.\end{align}
Concluimos que el método es consistente de orden $2$.

\subsection{Estabilidad}

La norma cuadrada de la función $u$
\begin{equation}
  \|u\|^{2}_2 = \int_{0}^{1}u^{2}(x)\,dx
\end{equation}
se puede discretizar como
\begin{equation}
  \|u\|^{2}_h = h\sum_{k=0}^{n+1}c_ku_k
  \qquad \text{ donde }
  c_1=c_2=\dots=c_n=1 
  \text{ y }
  c_0=c_{n+1}=\frac{1}{2}.
\end{equation}
El resultado de estabilidad que tenemos es
\begin{equation}
  \|u_h\|_h \leq \frac{1}{2} \|f_h\|_h
.\end{equation}


\section{Diferencias finitas en dimensión 2}

Supongamos que queremos encontrar una solución de la ecuación de
Poisson en 2 dimensiones
\begin{equation}
  -\Delta u = f
\end{equation}
en el interior cuadrado unitario $\Omega=[0,1]^{2}$, con condiciones
de frontera $u|\partial\Omega = g$.

Discretizaremos el espacio con puntos equiespaciados
\begin{equation}
  (x_i,y_j)=(ih,jh) \quad \text{donde} \quad i,j=0,1,\dots,n+1
.\end{equation}
Así, tenemos $n+1$ intervalos en cada lado, de modo que
$h=\frac{1}{n+1}$.
El operador laplaciano es $\Delta u = u_{xx}+u_{yy}$. Discretizaremos
la ecuación $-\Delta u = f$ como
\begin{equation}
  -\frac{u_{i-1,j} - 2u_{i,j} + u_{i+1,j}}{h^{2}}
  -\frac{u_{i,j-1} - 2u_{i,j} + u_{i,j+1}}{h^{2}}
  =
  f(x_i,y_j),
  \qquad
  1\leq i,j\leq n
,\end{equation}
y nos restringiremos al caso $u|_{\partial\Omega}=0$.
Las variables $u_{i,j}$ se pueden vectorizar. Este punto de vista se
toma en el libro Applied Numerical Linear Algebra, de James W.
Demmel.

\section{Método del elemento finito}

Consideremos el problema
\begin{equation}
  \left\{
    \begin{aligned}
      -(\alpha u')' + \beta u' + \gamma u &= f, \quad x\in (0,1),
      \\
      u(0) = 0, \quad u(1) = 0, &
    \end{aligned}
  \right.
\end{equation}
donde $\alpha,\beta,\gamma$ son continuas y $\alpha$ está acotada por
abajo por una constante positiva $\alpha_0$.
Para cualquier función $v$ continuamente diferenciable en $[0,1]$ con
$v(0)=v(1)=0$, tenemos
\begin{equation}
       \int_0^{1} \alpha u'v' + \int_0^{1} \beta u'v + \int_0^{1} \gamma u v
      = \int_0^{1} fv
.\end{equation}
Más generalmente, supongamos que $v$ está en el espacio
\begin{equation}
  V
  = H^{1}_0(0,1)
  = \{ v \in H^{1}(0,1) \mid v(0)=v(1)=1 \}.
\end{equation}
Aquí, $H^{1}(0,1)$ es el espacio de Sobolev
\begin{equation}
  H^{1}(0,1)
  =
  \{v\in L^{2}(0,1) \mid v' \text{ existe en } L^{2}(0,1) \}
,\end{equation}
donde $v'$ es la derivada débil de $v$. En otras palabras,
$H^{1}(0,1)$ consiste en los elementos $v\in L^{2}(0,1)$ tales que
existe una función $v'\in L^{2}(0,1)$ con
\begin{equation}
    \int_{0}^{1} \psi' v
    =
    - \int_{0}^{1} \psi v'
,\end{equation}
para toda $\psi\in C^{\infty}[0,1]$ con $\psi(0)=\psi(1)=0$.

Definamos el operador bilineal en $V=H^{1}_0(0,1)$ dado por
\begin{equation}
  a(u,v)
  = 
  \int_0^{1} \alpha u'v' + \int_0^{1} \beta u'v + \int_0^{1} \gamma u v
.\end{equation}
Así, podemos reformular el problema original como el de encontrar
$u\in V$ tal que
\begin{equation}
  \forall v\in V,\quad a(u,v) = (f,v)
.\end{equation}
El método de Galerkin consiste en fijar un subespacio de dimensión
finita $V_h$ de $V$ y encontrar $u_h\in V_h$ tal que
\begin{equation}
  \forall v\in V_h, \quad a(u_h,v) = (f,v)
.\end{equation}
Así, si $\phi_1,\dots,\phi_N$ es una base de $V_h$, entonces basta
pedir que
\begin{equation}
  a(u_h,\phi_i) = (f,\phi_i), \quad i=1,\dots,N
.\end{equation}
Expresando $u_h$ en términos de la misma base
\begin{equation}
  u_h = \sum_{j=1}^{N}u_j\phi_j
\end{equation}
tenemos
\begin{equation}
  \sum_{j=1}^{N} u_j a(\phi_j,\phi_i) = (f,\phi_i), \quad i=1,\dots,N
.\end{equation}
Este es un problema de dimensión finita, que se puede resolver por
métodos matriciales.

\begin{example}
  \begin{equation}
    a(\phi_j,\phi_i)
    = \int_{0}^{1} \alpha\phi_j'\phi_i'
    +
    \int_{0}^{1}\beta\phi_j'\phi_i
    +
    \int_{0}^{1}\gamma\phi_j\phi_i
  .\end{equation}
  Consideremos el caso en que $\phi_i$ ($i=1,\dots,n$) son las
  interpolaciones lineales a trozos definidas por
  $\phi_i(x_j)=\delta_{ij}$. Entonces $\phi_i$ tiene soporte en
  $[x_{i-1},x_{i+1}]$.
  La primera integral es
  \begin{equation}\label{eq:phiiphij}
    \int_{0}^{1} \alpha\phi_j'\phi_i'
  .\end{equation}
  Si suponemos que $\alpha$ es aproximadamente
  constante en $[x_{i-1},x_{i+1}]$, entonces resulta útil definir
  $\alpha_{i}=\alpha(x_i^{*})$, donde $x_i^{*}$ el punto
  medio de $[x_i,x_{i+1}]$. La integral \eqref{eq:phiiphij}
  se puede tratar por casos.
  \begin{align}
    \text{(para $i=j$)}\quad
    \int_{0}^{1} \alpha\phi_j'\phi_i'
    &= \int_{x_{i-1}}^{x_{i+1}}\alpha\phi_i'^{2} \\
    &= \int_{x_{i-1}}^{x_{i}}\alpha\phi_i'^{2}
    + \int_{x_{i}}^{x_{i+1}}\alpha\phi_i'^{2} \\
    &\approx \frac{\alpha_{i-1}}{h_{i-1}} + \frac{\alpha_i}{h_i}
  ,\end{align}
\end{example}

\subsection{Convergencia}

Estudiaremos la convergencia del método de elemento finito para el
problema
\begin{equation}
  \left\{
    \begin{aligned}
      & -u'' = f
      \\
      & u(0) = u(1)=0.
    \end{aligned}
  \right.
\end{equation}
Sean $u\in H_{0}^{1}(0,1)$ la solución exacta y $u_h\in V_h$ su
aproximación de elementos finitos usando las bases polinomiales a
trozos continuas en $[0,1]$ de grado $k\geq 1$. Suponga que, además,
$u\in H^{s}(0,1)$ para algún $s\geq 2$. Entonces
\begin{equation}
  \|u-u_h\|_{H^{1}_0(0,1)}
  \leq
  \frac{M}{\alpha_0} Ch^l\|u\|_{H^{(l+1)}(0,1)}
,\end{equation}
donde $l=\min(k,s-1)$. Más aún,
\begin{equation}
  \|u-u_h\|_{L^{2}(0,1)}
  \leq
  Ch^{l+1}\|u\|_{H^{l+1}(0,1)}
.\end{equation}

\chapter{Ecuaciones parabólicas e hiperbólicas}

Consideremos el problema
\begin{equation}
  \left\{
    \begin{aligned}
      & u_t = \nu u_{xx} + f(x,t), && x\in (0,1),
      \\
      & u(0,t) = u(1,t) = 0, && \text{(C.F.)}
      \\
      & u(x,0) = g(x), && \text{(C.I.)}
    \end{aligned}
  \right.
\end{equation}
donde $\nu$ es una constante, llamada constante de disipación.
Para aplicar el método de diferencias finitas, primero discretizamos
la variable espacial en puntos $x_0,\dots,x_{n+1}$, de modo
que obtenemos
\begin{align}
  \partial_t u_{i}
    &= \nu \frac{u_{i-1} -2u_{i} + u_{i+1}}{h^{2}} + f_i,
    \quad i=1,\dots,n
  \\
    u_i(0) = g_i
,\end{align}
donde $f_i(t)=f(x_i,t)$, $g_i=g(x_i)$ y definimos $u_0=u_1=0$ para
satisfacer las condiciones de frontera.
Con esto obtenemos un sistema de ecuaciones
\begin{align}
    \partial_t u_{1}
    &= -\frac{\nu }{h^{2}}(2u_{1} - u_{2}) + f_1,
  \\
    \partial_t u_{2}
    &= -\frac{\nu }{h^{2}}(-u_{1} +2u_{2} - u_{3}) + f_2,
  \\
    \vdots
  \\
    \partial_t u_{n-1}
    &= -\frac{\nu }{h^{2}}(-u_{n-2} +2u_{n-1} - u_{n}) + f_{n-1},
  \\
    \partial_t u_{n}
    &= -\frac{\nu }{h^{2}}(-u_{n-1} +2u_{n}) + f_n,
.\end{align}
Introduciremos la notación $U=(u_1,\dots,u_n)$, $F=(f_1,\dots,f_n)$ y
$G=(g_1,\dots,g_n)$, de modo que
\begin{equation}
  \left\{
    \begin{aligned}
      U' &= -\frac{\nu}{h^{2}}AU + F,
      \\
      U(0) &= G,
    \end{aligned}
  \right.
\end{equation}
donde
\begin{equation}
  A
  =
  \begin{bmatrix}
    2 & -1 \\
    -1 & 2 & -1 \\[-1mm]
       & -1 & 2 & \ddots \\[-1mm]
       & & \ddots & \ddots & \ddots \\[-1mm]
       & & & \ddots & 2 & -1 \\
       & & & & -1 & 2 & -1 \\
       & & & & & -1 & 2
  \end{bmatrix}
\end{equation}
Esta ecuación se puede resolver usando el llamado \emph{método
$\theta$}. Introduciendo $t^{k}=t_0+k\delta t$, $U^{k}=U(t^{k})$,
$F^{k}=F(t^{k})$, el método propone discretizar como
\begin{equation}
  \frac{U^{k+1}-U^k}{\Delta t}
  =
  \theta
  \left( -\frac{\nu}{h^{2}}AU^{k+1} + F^{k+1}, \right)
  +
  (1-\theta)
  \left( -\frac{\nu}{h^{2}}AU^{k} + F^{k} \right)
.\end{equation}
Luego, despejando $U_{k+1}$ tenemos
\begin{equation}
  \left( I
    +
    \Delta t
    \frac{\theta\nu}{h^{2}}A
  \right)
  U^{k+1}
  =
    \left(
      I-
      \Delta t
      (1-\theta)
      \frac{\nu}{h^{2}}A
    \right)
    U^k
    +
    \Delta t[
    (1-\theta)
    F^{k}
    +
    \theta
    F^{k+1}]
,\end{equation}
el cual se puede resolver explícitamente con valores iniciales
$U^{0}=U(0)=G$.
Escribiendo $M,N,P$ para estas tres matrices, tenemos
\begin{equation}
  \left\{
    \begin{aligned}
      M U^{k+1} &= NU^{k} + P
      \\
      U^{0} &= G
    \end{aligned}
  \right.
\end{equation}

\section{Estabilidad}

La estabilidad se estudia aplicando el método al problema de
prueba
\begin{equation}
  u_t = \nu u_{xx}
,\end{equation}
es decir, el problema con $f=0$.
Ahora procederemos por casos.
\begin{itemize}
  \item
    Si $\theta=0$, entonces el método se
    puede escribir explícitamente como
    \begin{equation}
      U^{k}
      =
        \left(
          I-
          \Delta t
          \frac{\nu}{h^{2}}A
        \right)^k
        G
    .\end{equation}
    Observemos que $U^{k}\to 0$ si, y solo si, el radio espectral de esta
    matríz satisface
    \begin{equation}
      \rho
        \left(
          I-
          \Delta t
          \frac{\nu}{h^{2}}A
        \right) < 1
    ,\end{equation}
    lo cual ocurre si, y solo si, $\Delta t < \frac{1}{2\nu}h^{2}$,
    de modo que el método es condicionalmente estable.

  \item
    Por otro lado, si $\theta =1$, entonces
    \begin{equation}
      U^{k}
      =
      \left( I
        +
        \Delta t
        \frac{\nu}{h^{2}}A
      \right)^{-1}
      G
    ,\end{equation}
    de modo que $U^{k}\to 0$ si, y solo si, el radio espectral de
    esta inversa es menor a $1$, o bien si
    \begin{equation}
      \rho
      \left( I
        +
        \Delta t
        \frac{\nu}{h^{2}}A
      \right)
      >1
    ,\end{equation}
    lo cual siempre sucede porque la matríz tiene autovalores
    positivos y mayores a $1$. Así, el método es incondicionalmente
    estable.
\end{itemize}

En general, el método $\theta$ es
\begin{itemize}
  \item
    incondicionalmente estable si $\frac{1}{2}\leq \theta\leq 1$.
  \item
    condicionalmente estable si $1\leq\theta<\frac{1}{2}$.
\end{itemize}
y el error de truncamiento local es
\begin{align}
  \tau_n(h)
  =
  \begin{cases}
    O(\Delta t + h^{2}) & \text{ si } \theta \neq 1 / 2 \\
    O((\Delta t)^{2} + h^{2}) & \text{ si } \theta = 1 / 2 \\
  \end{cases}
,\end{align}






\end{document}
